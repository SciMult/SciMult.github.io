<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SciMult">
  <meta name="keywords" content="pre-trained language model, mixture-of-experts, scientific text mining, contrastive learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SciMult</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-552M76N');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="icon" href="./static/images/logo_square.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <style>
    .err1marker {
      background-color: rgb(202,229,242);
      padding: 3px;
    }
    .err2marker {
      background-color: rgb(254,255,126);
      padding: 3px;
    }
    .err3marker {
      background-color: rgb(153,240,194);
      padding: 3px;
    }
  </style>

</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-552M76N" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yuzhimanhua.github.io/">Yu Zhang</a><sup style="color:#6fbf73">1*</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/site/hcheng2site/">Hao Cheng</a><sup style="color:#ed4b82">2*</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/zhihosh/">Zhihong Shen</a><sup style="color:#ffac33">3</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/xiaodl/">Xiaodong Liu</a><sup style="color:#ed4b82">2</sup>,</span>
            <span class="author-block">
              <a href="https://dblp.org/pid/13/1228.html">Ye-Yi Wang</a><sup style="color:#ffac33">3</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a><sup style="color:#ed4b82">2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73">1</sup>University of Illinois at Urbana-Champaign,</span>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>Microsoft Research</span><br>
            <span class="author-block"><sup style="color:#ffac33">3</sup>Microsoft Search, Assistant and Intelligence</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.14232"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yuzhimanhua/SciMult"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/yuz9yuz/SciMult"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Model</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1hoUAInDVO_UYnQiOOoVuBjwnVgY0BosO/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="./static/images/teaser.png" style="width:90%;">
            <p>
              We propose a task-aware specialization strategy, inpired by <b>the Mixture-of-Experts Transformer</b> architecture, to pre-train a scientific language model <b>SciMult</b> via contrastive multi-task learning. SciMult can be used for various scientific literature understanding tasks, including <b>coarse-grained/fine-grained paper classification</b>, <b>citation prediction</b>, <b>literature retrieval</b>, <b>paper recommendation</b>, <b>patient-to-article matching</b>, and so on.
            </p>
            <br>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Introduction</h2>

      <div class="content has-text-justified">
        <p>
          Scientific literature understanding plays a pivotal role in uncovering knowledge structures that facilitate scientific discovery. Recent studies highlight the effectiveness of pre-trained language models in scientific literature understanding tasks, especially when tuned through contrastive learning. However, there is a gap in jointly utilizing data across different tasks for language model pre-training.
        </p>
        <p>
          While multi-task learning is promising for improved parameter efficiency, sharing all parameters across tasks leads to undesirable task interference. To address this problem, we consider <b>the Mixture-of-Experts Transformer</b> architecture, which modifies the Transformer block in the language model to have multiple parallel sub-layers, each of which is dedicated for one task. When performing different tasks, the input will be routed to different sub-layers based on task types.
        </p>
        <p>
          We conduct a comprehensive empirical study using datasets from multiple sources (e.g., <b>MAPLE</b>, <b>SciDocs</b>, <b>SciRepEval</b>, <b>BEIR</b>, and <b>PMC-Patients</b>) for evaluating various scientific literature understanding tasks. For each task, models will be tested on not only <b>in-domain</b> but also <b>cross-domain</b> evaluation datasets. Specifically, for extreme multi-label text classification, models trained on computer science and biomedicine papers will be tested in the geography and psychology fields; for link prediction, models trained on citation signals need to be evaluated on patient-to-patient matching and paper recommendation; for literature retrieval, models will be tested on datasets specific to COVID-19 or related to claim verification which are not seen during pre-training. <b>Experimental results show that SciMult outperforms competitive scientific pre-trained language models on most datasets and achieves the new state-of-the-art performance on the PMC-Patients leaderboard.</b>
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Evaluation Datasets</h2>

      <div class="content has-text-justified">
        <p>
          The preprocessed evaluation datasets can be downloaded from <a href="https://drive.google.com/file/d/1hoUAInDVO_UYnQiOOoVuBjwnVgY0BosO/view?usp=drive_link" target="_blank">here</a>.
        </p>
        <p>
          <b>Disclaimer:</b> The aggregate version is released under the ODC-By v1.0 License. By downloading this version you acknowledge that you have read and agreed to all the terms in this license. Similar to Tensorflow datasets or Hugging Face's datasets library, we just downloaded and prepared public datasets. We only distribute these datasets in a specific format, but we do not vouch for their quality or fairness, or claim that you have the license to use the dataset. It remains the user's responsibility to determine whether you as a user have permission to use the dataset under the dataset's license and to cite the right owner of the dataset.
        </p>
        <p>
          More details about each constituent dataset can be found in our <a href="https://github.com/yuzhimanhua/SciMult#datasets" target="_blank">GitHub repository</a>.
        </p>
      </div>
      <img src="static/images/dataset1.png" style="width:90%;">
      <img src="static/images/dataset2.png" style="width:90%;">
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance</h2>
        <p>Comparisons between <b>SciMult</b> and state-of-the-art scientific pre-trained language models (e.g., <b>SciBERT</b>, <b>PubMedBERT</b>, <b>SPECTER</b>, <b>SciNCL</b>, and <b>SPECTER 2.0</b>) in each task.</p>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/1_class_fine.png" alt="fine-grained paper classification" height="5em"/>
              <p><b>Fine-Grained Paper Classification</b></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/2_class_coarse.png" alt="coarse-grained paper classification" height="5em"/>
              <p><b>Coarse-Grained Paper Classification</b></p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2023pre,
      title={Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding},
      author={Zhang, Yu and Cheng, Hao and Shen, Zhihong and Liu, Xiaodong and Wang, Ye-Yi and Gao, Jianfeng},
      journal={arXiv preprint arXiv:2305.14232},
      year={2023}
    }
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
