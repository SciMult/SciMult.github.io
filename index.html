<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FineGrainedRLHF">
  <meta name="keywords" content="RLHF, language model, long-form question generation, reinforcement learning from human feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fine-Grained RLHF</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-552M76N');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_square.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .err1marker {
      background-color: rgb(202,229,242);
      padding: 3px;
    }
    .err2marker {
      background-color: rgb(254,255,126);
      padding: 3px;
    }
    .err3marker {
      background-color: rgb(153,240,194);
      padding: 3px;
    }
  </style>

</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-552M76N" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ellenmellon.github.io/">Zeqiu Wu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://yushi-hu.github.io/">Yushi Hu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://weijia-shi.netlify.app/">Weijia Shi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://webdocs.cs.ualberta.ca/~dziri/">Nouha Dziri</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.alanesuhr.com/">Alane Suhr</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://prithvirajva.com/">Prithviraj Ammanabrolu</a><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="http://nasmith.github.io/">Noah A. Smith</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.ece.uw.edu/ostendorf/">Mari Ostendorf</a><sup>1</sup>,
            </span>
              <span class="author-block">
                    <a href="https://homes.cs.washington.edu/~hannaneh/">Hannaneh Hajishirzi</a><sup>1,2</sup>
                    </span>
            

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Allen Institute for AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.01693"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/allenai/FineGrainedRLHF"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code: FineGrainedRLHF</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/allenai/FineGrainedRLHF/tree/main/tasks/qa_feedback/data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>QA-Feedback Dataset</span>
                  </a>
              </span>


            </div>

                          

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser-smaller.jpg" alt="tifa teaser">
            <h2 class="subtitle">
              <p>We propose <span style="color: red;"><b>Fine-Grained RLHF</b></span>, a framework that enables training and learning
              from reward functions that are fine-grained in two respects:</p>
              <p>
                (1) density, providing a reward after every segment (e.g., a sentence) is generated
              </p>
                (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness).
            </h2>

      
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">What are Fine-Grained Rewards?</h2>
      <div class="content has-text-justified">
        <br>
        <p>
          Prior work in RLHF focused on collecting human preferences regarding the overall quality of language model
          (LM) outputs. However, this type of holistic feedback
          offers limited information.
          In our paper, we introduce <span style="color: red;"><b>fine-grained human feedback</b></span> (e.g.,
          which sub-sentence is irrelevant, which sentence is not truthful, which sentence is toxic) as an explicit training signal.
        </p>
        <p>
          Our rewards are fine-grained in two aspects:
        </p>
        <p>
          <span style="color: red;"><b>(a) Density</b></span>: We provide a reward after each segment (e.g., a sentence)
          is
          generated, similar to OpenAI's "step-by-step process reward". We found that this approach is more informative than
          holistic
          feedback and, thus, more effective for RL.
        </p>

        <p>
          <span style="color: red;"><b>(b) Multiple reward models associated with different feedback types</b></span>:
          We employ
          multiple reward models to capture different types of feedback (e.g., factual inaccuracy, irrelevance, and
          information incompleteness).
          Interestingly, we observed that these reward models both complement and compete with each other. By adjusting
          the
          weights of the reward models, we can control the balance between the different types of feedback and <span
            style="color: red;"><b>tailor the LM for different tasks</b></span> according to specific needs.
          For instance, some users may prefer short and concise outputs, while others may seek longer and more detailed
          responses.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Updates</h2>
      <div class="content has-text-justified">
        <p>
          <b>2023/07/04</b> Our codebase, FineGrainedRLHF, is released!<br>
          <b>2023/06/05</b> QA-Feedback, our long-form QA dataset with human preferences + fine-grained feedback, is
          released!<br>
          <b>2023/06/05</b> Paper is released!<br>
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or
          irrelevant outputs. Reinforcement learning from human feedback (RLHF)—where human preference judgments on LM outputs are
          transformed into a learning signal—has recently shown promise in addressing these issues. 
          However, such holistic
          feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced
          user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback
          (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce
          FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two
          respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating
          multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and
          information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how
          learning with such reward functions leads to improved performance, supported by both automatic and human evaluation.
          Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models.
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Task 1: Detoxification</h2>

      <div class="content has-text-justified">
        <p>
          The task of detoxification aims to reduce the toxicity in the model generation. We use Perspective API to measure toxicity. It returns a toxicity value between 0 (not toxic) and 1
          (toxic).

        </p>
      </div>


      <img src="static/images/toxicity_demo.png" style="width:60%;">
      <div class="content has-text-justified">
        <p>
          We compare two kinds of rewards:
        </p>
        <p>
          (a) <b>Holistic Rewards for (non-)Toxicity</b>: We use 1-Perspective(y) as the reward
        </p>
          <p>
          (b) <b>Sentence-level (Fine-Grained) Rewards for (non-)Toxicity</b>: We query the API after the model generates each sentence instead of generating the full sequence. For each generated
          sentence, we use -&Delta;(Perspective(y)) as the reward for the sentence (i.e. how much toxicity is changed from generating the current sentence).
          </p>
      </div>
      <img src="static/images/toxicity_results.png" style="width:100%;">
      <br><br>
      <div class="content has-text-justified">
        <p>
          Table 1 shows that Our Fine-Grained RLHF with sentence-level
          fine-grained reward attains the lowest toxicity and perplexity among all methods, while maintaining
          a similar level of diversity. Figure 2 shows that learning from denser fine-grained reward is more <b>sample
            efficient</b> than holistic reward. One explanation is that fine-grained reward locates where the toxic content is, which is a
          stronger training signal compared with a scalar reward for the whole text.
        </p>
      </div>


    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Task 2: Long-Form Question Answering</h2>

      <div class="content has-text-justified">
        <p>
          We collect <span style="color: red;"><b>QA-Feeback</b></span>, a dataset of long-form question answering, with human preferences and fine-grained feedback.
          QA-Feedback is based on ASQA, a dataset that focuses on answering ambiguous factoid questions.
        </p>
        <p>
          There are three types of fine-grained human feedback, and we train a fine-grained reward model for each of them:
        </p>
        <p>
          <span class="err1marker">C1: irrelevance, repetition, and incoherence (rel.)</span>; The reward model has the density level of sub-sentences; i.e., returns a score for each sub-sentence. 
          If the sub-sentence is irrelevant, repetitive, or incoherent, the reward is -1; otherwise, the reward is +1.
        </p>
        <p>
          <span class="err2marker">C2: incorrect or unverifiable facts (fact.)</span>; The reward model has the density level of sentences; i.e., returns a score for each sentence.
          If the sentence has any factual error, the reward is -1; otherwise, the reward is +1.
        </p>
        <p>
          <span class="err3marker">C3: incomplete information (comp.)</span>; The reward model checks if the response is complete and covers all the information in the reference passages that are related to the question.
          This reward model gives one reward for the whole response. 
        </p>
      </div>

    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Fine-Grained Human Evaluation</h2>

      <div class="content has-text-justified">
      
      <p>We compare our <b>Fine-Grained RLHF</b> with the following baselines:</p>
        <p><b>SFT</b>: The supervised finetuning model (trained on 1K training examples) that is used as the initial policy for our RLHF experiments.</p>
        <p><b>Pref. RLHF</b>: The baseline RLHF model that uses holistic reward.</p>  
        <p><b>SFT-Full</b>: We finetune LM with human-written responses (provided by ASQA) of all training examples and denote this model as SFT-Full. Notice that each gold response takes 15 min to annotate (according to ASQA), 
      which takes much longer time than our feedback annotation (6 min).</p>
        </p>
      </div>


      <img src="static/images/human_eval.png" style="width:100%;">
      <div class="content has-text-justified">
        <p> 
          Human evaluation shows that our <b>Fine-Grained RLHF</b> outperforms SFT and Preference RLHF on all error types. 
          Also, RLHF (both preference-based and fine-grained) are particularly effective in reducing factual errors.
        </p>
      </div>

    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Customize LM behaviors</h2>
      <img src="static/images/customization.png" style="width:100%;">
      <div class="content has-text-justified">
        By changing the weight of the <span class="err1marker"><b>Relevance reward model</b></span>, and keeping the weight of the other two reward models fixed,
        we can customize how detailed and lengthy the LM responses would be.
        Here we compare the outputs of three LMs, trained with different reward model combinations.
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Fine-Grained reward models both complement and compete with each other</h2>
      <img src="static/images/analysis.png" style="width:100%;">
      <br><br>
      <div class="content has-text-justified">
        We find that there is a trade-off between the three reward models. 
        <span class="err1marker"><b>Relevance RM</b></span> prefers shorter and more concise responses, while <span class="err3marker"><b>Info Completeness RM</b></span> prefers longer and more informative responses.
        Thus, these two rewards compete against each other during training and eventually reach a balance.
        Meanwhile, <span class="err2marker"><b>Factuality RM</b></span> continuously improves the factual correctness of the response.
        Finally, removing any one of the reward models will degrade the performance.

      </div>
    </div>
  </div>
</section>





<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wu2023fine,
    title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},
    author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith,
    Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
    journal={arXiv preprint arXiv:2306.01693},
    year={2023}
    }
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
