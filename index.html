<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SciMult">
  <meta name="keywords" content="pre-trained language model, mixture-of-experts, scientific text mining, contrastive learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SciMult</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-552M76N');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_square.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .err1marker {
      background-color: rgb(202,229,242);
      padding: 3px;
    }
    .err2marker {
      background-color: rgb(254,255,126);
      padding: 3px;
    }
    .err3marker {
      background-color: rgb(153,240,194);
      padding: 3px;
    }
  </style>

</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-552M76N" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yuzhimanhua.github.io/">Yu Zhang</a><sup style="color:#6fbf73">1*</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/site/hcheng2site/">Hao Cheng</a><sup style="color:#ed4b82">2*</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/zhihosh/">Zhihong Shen</a><sup style="color:#ffac33">3</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/xiaodl/">Xiaodong Liu</a><sup style="color:#ed4b82">2</sup>,</span>
            <span class="author-block">
              <a href="https://dblp.org/pid/13/1228.html">Ye-Yi Wang</a><sup style="color:#ffac33">3</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a><sup style="color:#ed4b82">2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73">1</sup>University of Illinois at Urbana-Champaign,</span>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>Microsoft Research</span><br>
            <span class="author-block"><sup style="color:#ffac33">3</sup>Microsoft Search, Assistant and Intelligence</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.14232"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yuzhimanhua/SciMult"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/yuz9yuz/SciMult"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Model</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1hoUAInDVO_UYnQiOOoVuBjwnVgY0BosO/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="./static/images/teaser.png" style="width:90%;">
            <p>
              We propose a task-aware specialization strategy, inpired by <b>the Mixture-of-Experts Transformer</b> architecture, to pre-train a scientific language model <b>SciMult</b> via contrastive multi-task learning. SciMult can be used for various scientific literature understanding tasks, including <b>coarse-grained/fine-grained paper classification</b>, <b>citation prediction</b>, <b>literature retrieval</b>, <b>paper recommendation</b>, <b>patient-to-article matching</b>, and so on.
            </p>

    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Introduction</h2>
      <div class="content has-text-justified">
        <br>
        <p>
          Scientific literature understanding plays a pivotal role in uncovering knowledge structures that facilitate scientific discovery. Recent studies highlight the effectiveness of pre-trained language models in scientific literature understanding tasks, especially when tuned through contrastive learning. However, there is a gap in jointly utilizing data across different tasks for language model pre-training.
        </p>
        <p>
          While multi-task learning is promising for improved parameter efficiency, sharing all parameters across tasks leads to undesirable task interference. To address this problem, we consider <b>the Mixture-of-Experts Transformer</b> architecture, which modifies the Transformer block in the language model to have multiple parallel sub-layers, each of which is dedicated for one task. When performing different tasks, the input will be routed to different sub-layers based on task types.
        </p>
        <p>
          We conduct a comprehensive empirical study using datasets from multiple sources (e.g., <b>MAPLE</b>, <b>SciDocs</b>, <b>SciRepEval</b>, <b>BEIR</b>, and <b>PMC-Patients</b>) for evaluating various scientific literature understanding tasks. For each task, models will be tested on not only <b>in-domain</b> but also <b>cross-domain</b> evaluation datasets. Specifically, for extreme multi-label text classification, models trained on computer science and biomedicine papers will be tested in the geography and psychology fields; for link prediction, models trained on citation signals need to be evaluated on patient-to-patient matching and paper recommendation; for literature retrieval, models will be tested on datasets specific to COVID-19 or related to claim verification which are not seen during pre-training. <b>Experimental results show that SciMult outperforms competitive scientific pre-trained language models on most datasets and achieves the new state-of-the-art performance on the PMC-Patients leaderboard.</b>
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Evaluation Datasets</h2>

      <div class="content has-text-justified">
        <p>
          The preprocessed evaluation datasets can be downloaded from <a href="https://drive.google.com/file/d/1hoUAInDVO_UYnQiOOoVuBjwnVgY0BosO/view?usp=drive_link" target="_blank">here</a>.
        </p>
        <p>
          <b>Disclaimer:</b> The aggregate version is released under the ODC-By v1.0 License. By downloading this version you acknowledge that you have read and agreed to all the terms in this license. Similar to Tensorflow datasets or Hugging Face's datasets library, we just downloaded and prepared public datasets. We only distribute these datasets in a specific format, but we do not vouch for their quality or fairness, or claim that you have the license to use the dataset. It remains the user's responsibility to determine whether you as a user have permission to use the dataset under the dataset's license and to cite the right owner of the dataset.
        </p>
        <p>
          More details about each constituent dataset can be found in our <a href="https://github.com/yuzhimanhua/SciMult#datasets" target="_blank">GitHub repository</a>.
        </p>
      </div>
      <img src="static/images/dataset1.png" style="width:90%;">
      <img src="static/images/dataset2.png" style="width:90%;">
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Task 2: Long-Form Question Answering</h2>

      <div class="content has-text-justified">
        <p>
          We collect <span style="color: red;"><b>QA-Feeback</b></span>, a dataset of long-form question answering, with human preferences and fine-grained feedback.
          QA-Feedback is based on ASQA, a dataset that focuses on answering ambiguous factoid questions.
        </p>
        <p>
          There are three types of fine-grained human feedback, and we train a fine-grained reward model for each of them:
        </p>
        <p>
          <span class="err1marker">C1: irrelevance, repetition, and incoherence (rel.)</span>; The reward model has the density level of sub-sentences; i.e., returns a score for each sub-sentence. 
          If the sub-sentence is irrelevant, repetitive, or incoherent, the reward is -1; otherwise, the reward is +1.
        </p>
        <p>
          <span class="err2marker">C2: incorrect or unverifiable facts (fact.)</span>; The reward model has the density level of sentences; i.e., returns a score for each sentence.
          If the sentence has any factual error, the reward is -1; otherwise, the reward is +1.
        </p>
        <p>
          <span class="err3marker">C3: incomplete information (comp.)</span>; The reward model checks if the response is complete and covers all the information in the reference passages that are related to the question.
          This reward model gives one reward for the whole response. 
        </p>
      </div>

    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Fine-Grained Human Evaluation</h2>

      <div class="content has-text-justified">
      
      <p>We compare our <b>Fine-Grained RLHF</b> with the following baselines:</p>
        <p><b>SFT</b>: The supervised finetuning model (trained on 1K training examples) that is used as the initial policy for our RLHF experiments.</p>
        <p><b>Pref. RLHF</b>: The baseline RLHF model that uses holistic reward.</p>  
        <p><b>SFT-Full</b>: We finetune LM with human-written responses (provided by ASQA) of all training examples and denote this model as SFT-Full. Notice that each gold response takes 15 min to annotate (according to ASQA), 
      which takes much longer time than our feedback annotation (6 min).</p>
        </p>
      </div>


      <img src="static/images/human_eval.png" style="width:100%;">
      <div class="content has-text-justified">
        <p> 
          Human evaluation shows that our <b>Fine-Grained RLHF</b> outperforms SFT and Preference RLHF on all error types. 
          Also, RLHF (both preference-based and fine-grained) are particularly effective in reducing factual errors.
        </p>
      </div>

    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Customize LM behaviors</h2>
      <img src="static/images/customization.png" style="width:100%;">
      <div class="content has-text-justified">
        By changing the weight of the <span class="err1marker"><b>Relevance reward model</b></span>, and keeping the weight of the other two reward models fixed,
        we can customize how detailed and lengthy the LM responses would be.
        Here we compare the outputs of three LMs, trained with different reward model combinations.
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Fine-Grained reward models both complement and compete with each other</h2>
      <img src="static/images/analysis.png" style="width:100%;">
      <br><br>
      <div class="content has-text-justified">
        We find that there is a trade-off between the three reward models. 
        <span class="err1marker"><b>Relevance RM</b></span> prefers shorter and more concise responses, while <span class="err3marker"><b>Info Completeness RM</b></span> prefers longer and more informative responses.
        Thus, these two rewards compete against each other during training and eventually reach a balance.
        Meanwhile, <span class="err2marker"><b>Factuality RM</b></span> continuously improves the factual correctness of the response.
        Finally, removing any one of the reward models will degrade the performance.

      </div>
    </div>
  </div>
</section>





<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wu2023fine,
    title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},
    author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith,
    Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
    journal={arXiv preprint arXiv:2306.01693},
    year={2023}
    }
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
